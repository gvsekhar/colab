{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c284f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from transformers import AutoModel, AutoTokenizer,AutoModelForSeq2SeqLM, AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d342268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTokenizerWrapper:\n",
    "    def __init__(self, model_name, sent_transform,ner):\n",
    "        # Initialize the tokenizer and model for qa\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Initialize question generation model\n",
    "        self.nlp = pipeline(\"question-generation\", model=\"valhalla/t5-small-qg-prepend\", qg_format=\"prepend\")\n",
    "\n",
    "        self.sent_tran = SentenceTransformer(sent_transform)\n",
    "        #'bert-base-nli-mean-tokens')\n",
    "\n",
    "        ## Initialize the tokenizer and model for ner \n",
    "        self.tokenizer_ner = AutoTokenizer.from_pretrained(ner)\n",
    "        self.model_ner  = AutoModelForTokenClassification.from_pretrained(ner)                  \n",
    "\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_qg_model(self):\n",
    "        return self.nlp\n",
    "\n",
    "    def get_sentence_transfomer(self):\n",
    "        return self.sent_tran\n",
    "    \n",
    "    def get_ner_model(self):\n",
    "        return self.model_ner\n",
    "    \n",
    "    def get_ner_tokenizer(self):\n",
    "        return self.tokenizer_ner\n",
    "\n",
    "def generate_df(text,summary):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    print(\" in generate_df \")\n",
    "    response = nlp(text)\n",
    "    df = pd.DataFrame.from_records(response)\n",
    "    df['text'] = text\n",
    "    df['summary']= summary\n",
    "    return(df)\n",
    "\n",
    "def generate_answer(question, context,  model, tokenizer):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    print(\" in generate_answer \")\n",
    "    prompt = f\"Use the following context to answer questions: \\n Context :{context} \\n Question: {question}\"\n",
    "    inputs = tokenizer( prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return(response)\n",
    "\n",
    "def combine_entities(entities):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Initialize variables\n",
    "    combined_entities = []\n",
    "    current_entity = None\n",
    "    first_int= True\n",
    "\n",
    "    # Iterate through the entities\n",
    "    for entity in entities:\n",
    "        #print(entity)\n",
    "        entity_prefix , entity_type  = entity['entity'].split('-')\n",
    "\n",
    "        #print(entity_type, entity_prefix)\n",
    "        if entity_prefix == 'B':\n",
    "            \n",
    "            # If it's the beginning of an entity, create a new entity\n",
    "            if current_entity:\n",
    "                combined_entities.append(current_entity)\n",
    "            current_entity = {'entity': entity_type, 'word': entity['word']}\n",
    "            \n",
    "        elif entity_prefix == 'I':\n",
    "            # If it's inside an entity, append the word to the current entity\n",
    "            if current_entity:\n",
    "                if(entity['word'].startswith(\"##\")):\n",
    "                    current_entity['word'] += ''+ entity['word'].replace(\"##\",'')\n",
    "                else:\n",
    "                    current_entity['word'] += ' '+ entity['word']\n",
    "        else:\n",
    "            # If it's outside an entity, add the current entity to the result and reset it\n",
    "            if current_entity:\n",
    "                combined_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "    # Add the last entity if it exists\n",
    "    if current_entity:\n",
    "        combined_entities.append(current_entity)\n",
    "\n",
    "    # Print the combined entities\n",
    "    return combined_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb85e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_matches(id,text,summary,ner_nlp):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    ner_results = ner_nlp(text)\n",
    "    combined_entities = combine_entities(ner_results)\n",
    "    df_combined_entities = pd.DataFrame.from_records(combined_entities)\n",
    "\n",
    "    ner_results_summary= ner_nlp(summary)\n",
    "    combined_entities_summary = combine_entities(ner_results_summary)\n",
    "    df_combined_entities_summary = pd.DataFrame.from_records(combined_entities_summary)\n",
    "    #print(df_combined_entities_summary)\n",
    "    items_not_in_summary = [item for item in df_combined_entities['word'] if item not in df_combined_entities_summary['word'] ]\n",
    "    items_not_in_original = [item for item in df_combined_entities_summary['word'] if item not in df_combined_entities['word'] ]\n",
    "    items_present_in_both = [item for item in df_combined_entities['word'] if item in df_combined_entities_summary['word'] ]\n",
    "\n",
    "    ind1= len(items_not_in_summary)\n",
    "    ind2=len(items_not_in_original)\n",
    "    ind3= len(items_present_in_both)\n",
    "    \n",
    "    text_entities=','.join(df_combined_entities['word'] )\n",
    "    summary_entities = \",\".join( df_combined_entities_summary['word'])\n",
    "    \n",
    "    return({\n",
    "        'id':id,\n",
    "        \"text_entities\":text_entities,\n",
    "        \"summary_entities\":summary_entities,\n",
    "        \"items_not_in_summary_cnt\":ind1,\n",
    "        \"items_not_in_summary\" : items_not_in_summary,\n",
    "        \"items_not_in_original_cnt\" :ind2,\n",
    "        \"items_not_in_original\" :items_not_in_original,\n",
    "        \"items_present_in_both_cnt\":ind3,\n",
    "        \"items_present_in_both\" : items_present_in_both\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf6704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_verification_questions( data, tokenizer, model ,nlp , sent_tran):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    print(\" in process_csv \")\n",
    "    try:\n",
    "        \n",
    "        l = []\n",
    "        for i, row in data.iterrows():\n",
    "            \n",
    "            df = generate_df(row['text'],row['summary'])\n",
    "            df['response_text']= [' '.join(generate_answer(question, text , model, tokenizer)) for question, text in zip(df['question'],df['text'])]\n",
    "            df['response_summary']= [' '.join(generate_answer(question, summary ,  model, tokenizer)) for question, summary in zip(df['question'],df['summary'])]\n",
    "            temp1 = sent_tran.encode(df['response_text'])\n",
    "            temp2 = sent_tran.encode(df['response_summary'])\n",
    "            df['cosine_score'] = [cosine_similarity(x.reshape(1, -1),y.reshape(1, -1))[0][0] for x, y in zip(temp1,temp2)]\n",
    "            df['id'] = row['id']\n",
    "            l.append(df)\n",
    "        return(l)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred: {e}\")    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_ner(data,model_ner, tokenizer_ner):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from transformers import pipeline as p\n",
    "\n",
    "    l =[]\n",
    "    ner_nlp = p(\"ner\", model=model_ner, tokenizer=tokenizer_ner)\n",
    "    for i, row in data.iterrows():\n",
    "        l.append(get_entity_matches(row['id'],row['text'],row['summary'], ner_nlp))\n",
    "    return (pd.DataFrame(l))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python haludetect.py path_to_csv_file\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        file_path = sys.argv[1]\n",
    "        print(\"Processing..\")\n",
    "    \n",
    "        model_name = \"google/flan-t5-xl\"\n",
    "        sent_transform = \"bert-base-nli-mean-tokens\"\n",
    "        model_ner = \"dslim/bert-large-NER\"\n",
    "        \n",
    "        wrapper = ModelTokenizerWrapper(model_name, sent_transform,model_ner)\n",
    "        \n",
    "        tokenizer = wrapper.get_tokenizer()\n",
    "        model = wrapper.get_model()\n",
    "        \n",
    "        nlp = wrapper.get_qg_model()\n",
    "        sent_tran = wrapper.get_sentence_transfomer()\n",
    "\n",
    "        model_ner = wrapper.get_ner_model()\n",
    "        tokenizer_ner = wrapper.get_ner_tokenizer()\n",
    "    \n",
    "        data = pd.read_csv(file_path)\n",
    "        data['id'] = range(1,len(data)+1)\n",
    "\n",
    "        ## Method1\n",
    "        output_1 = process_csv_verification_questions(data, tokenizer, model ,nlp , sent_tran) \n",
    "        df1 = pd.concat(output_1)\n",
    "        print(df1.columns)\n",
    "\n",
    "        ##Method2\n",
    "        df2 = process_csv_ner(data,model_ner, tokenizer_ner)\n",
    "    \n",
    "        df1.to_csv(\"output_method1.csv\", index=False)\n",
    "        df2.to_csv(\"output_method2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "356732b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 41943040 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24972\\3748475799.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mmodel_ner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"dslim/bert-large-NER\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mwrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelTokenizerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_ner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24972\\3748475799.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name, sent_transform, ner)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Initialize the tokenizer and model for qa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Initialize question generation model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m         raise ValueError(\n\u001b[0;32m    448\u001b[0m             \u001b[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2177\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2178\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m         \u001b[0mencoder_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[0mdecoder_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, embed_tokens)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         self.block = nn.ModuleList(\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         self.block = nn.ModuleList(\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mT5Block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_relative_attention_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5LayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT5LayerCrossAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT5LayerFF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     def forward(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gated_act\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5DenseGatedActDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5DenseActDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_ff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_ff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_ff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_act_fn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 41943040 bytes."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForTokenClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ModelTokenizerWrapper:\n",
    "    def __init__(self, model_name, sent_transform, ner):\n",
    "        # Initialize the tokenizer and model for qa\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Initialize question generation model\n",
    "        self.nlp = pipeline(\"question-answering'\", model=\"valhalla/t5-small-qg-prepend\", qg_format=\"prepend\")\n",
    "\n",
    "        self.sent_tran = SentenceTransformer(sent_transform)\n",
    "\n",
    "        ## Initialize the tokenizer and model for ner \n",
    "        self.tokenizer_ner = AutoTokenizer.from_pretrained(ner)\n",
    "        self.model_ner  = AutoModelForTokenClassification.from_pretrained(ner)                  \n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_qg_model(self):\n",
    "        return self.nlp\n",
    "\n",
    "    def get_sentence_transfomer(self):\n",
    "        return self.sent_tran\n",
    "    \n",
    "    def get_ner_model(self):\n",
    "        return self.model_ner\n",
    "    \n",
    "    def get_ner_tokenizer(self):\n",
    "        return self.tokenizer_ner\n",
    "\n",
    "def generate_df(text, summary, nlp):\n",
    "    response = nlp(text)\n",
    "    df = pd.DataFrame.from_records(response)\n",
    "    df['text'] = text\n",
    "    df['summary'] = summary\n",
    "    return df\n",
    "\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    prompt = f\"Use the following context to answer questions: \\n Context :{context} \\n Question: {question}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def combine_entities(entities):\n",
    "    combined_entities = []\n",
    "    current_entity = None\n",
    "    first_int = True\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_prefix, entity_type = entity['entity'].split('-')\n",
    "\n",
    "        if entity_prefix == 'B':\n",
    "            if current_entity:\n",
    "                combined_entities.append(current_entity)\n",
    "            current_entity = {'entity': entity_type, 'word': entity['word']}\n",
    "            \n",
    "        elif entity_prefix == 'I':\n",
    "            if current_entity:\n",
    "                if entity['word'].startswith(\"##\"):\n",
    "                    current_entity['word'] += '' + entity['word'].replace(\"##\", '')\n",
    "                else:\n",
    "                    current_entity['word'] += ' ' + entity['word']\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append(current_entity)\n",
    "\n",
    "    return combined_entities\n",
    "\n",
    "def get_entity_matches(id, text, summary, ner_nlp):\n",
    "    ner_results = ner_nlp(text)\n",
    "    combined_entities = combine_entities(ner_results)\n",
    "    df_combined_entities = pd.DataFrame.from_records(combined_entities)\n",
    "\n",
    "    ner_results_summary = ner_nlp(summary)\n",
    "    combined_entities_summary = combine_entities(ner_results_summary)\n",
    "    df_combined_entities_summary = pd.DataFrame.from_records(combined_entities_summary)\n",
    "\n",
    "    items_not_in_summary = [item for item in df_combined_entities['word'] if item not in df_combined_entities_summary['word']]\n",
    "    items_not_in_original = [item for item in df_combined_entities_summary['word'] if item not in df_combined_entities['word']]\n",
    "    items_present_in_both = [item for item in df_combined_entities['word'] if item in df_combined_entities_summary['word']]\n",
    "\n",
    "    ind1 = len(items_not_in_summary)\n",
    "    ind2 = len(items_not_in_original)\n",
    "    ind3 = len(items_present_in_both)\n",
    "    \n",
    "    text_entities = ','.join(df_combined_entities['word'])\n",
    "    summary_entities = \",\".join(df_combined_entities_summary['word'])\n",
    "    \n",
    "    return {\n",
    "        'id': id,\n",
    "        \"text_entities\": text_entities,\n",
    "        \"summary_entities\": summary_entities,\n",
    "        \"items_not_in_summary_cnt\": ind1,\n",
    "        \"items_not_in_summary\": items_not_in_summary,\n",
    "        \"items_not_in_original_cnt\": ind2,\n",
    "        \"items_not_in_original\": items_not_in_original,\n",
    "        \"items_present_in_both_cnt\": ind3,\n",
    "        \"items_present_in_both\": items_present_in_both\n",
    "    }\n",
    "\n",
    "def process_csv_verification_questions(data, tokenizer, model, nlp, sent_tran, file_path):\n",
    "    try:\n",
    "        l = []\n",
    "        for i, row in data.iterrows():\n",
    "            df = generate_df(row['text'], row['summary'], nlp)\n",
    "            df['response_text'] = [' '.join(generate_answer(question, text, model, tokenizer)) for question, text in zip(df['question'], df['text'])]\n",
    "            df['response_summary'] = [' '.join(generate_answer(question, summary, model, tokenizer)) for question, summary in zip(df['question'], df['summary'])]\n",
    "            temp1 = sent_tran.encode(df['response_text'])\n",
    "            temp2 = sent_tran.encode(df['response_summary'])\n",
    "            df['cosine_score'] = [cosine_similarity(x.reshape(1, -1), y.reshape(1, -1))[0][0] for x, y in zip(temp1, temp2)]\n",
    "            df['id'] = row['id']\n",
    "            l.append(df)\n",
    "        return l\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred: {e}\")    \n",
    "\n",
    "def process_csv_ner(data, model_ner, tokenizer_ner):\n",
    "    l = []\n",
    "    ner_nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)\n",
    "    for i, row in data.iterrows():\n",
    "        l.append(get_entity_matches(row['id'], row['text'], row['summary'], ner_nlp))\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "file_path = \"C:/Users/vijaya.sekhar/Desktop/accelarators/hallucination/test.csv\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) != 2:\n",
    "#         print(\"Usage: python haludetect.py path_to_csv_file\")\n",
    "#         sys.exit(1)\n",
    "#     else:\n",
    "#         file_path = sys.argv[1]\n",
    "#         print(\"Processing..\")\n",
    "    \n",
    "        model_name = \"google/flan-t5-xl\"\n",
    "        sent_transform = \"bert-base-nli-mean-tokens\"\n",
    "        model_ner = \"dslim/bert-large-NER\"\n",
    "        \n",
    "        wrapper = ModelTokenizerWrapper(model_name, sent_transform, model_ner)\n",
    "        \n",
    "        tokenizer = wrapper.get_tokenizer()\n",
    "        model = wrapper.get_model()\n",
    "        \n",
    "        nlp = wrapper.get_qg_model()\n",
    "        sent_tran = wrapper.get_sentence_transfomer()\n",
    "\n",
    "        model_ner = wrapper.get_ner_model()\n",
    "        tokenizer_ner = wrapper.get_ner_tokenizer()\n",
    "    \n",
    "        data = pd.read_csv(file_path)\n",
    "        data['id'] = range(1, len(data) + 1)\n",
    "\n",
    "        ## Method1\n",
    "        output_1 = process_csv_verification_questions(data, tokenizer, model, nlp, sent_tran, file_path)\n",
    "        df1 = pd.concat(output_1)\n",
    "        print(df1.columns)\n",
    "\n",
    "        ##Method2\n",
    "        df2 = process_csv_ner(data, model_ner, tokenizer_ner)\n",
    "    \n",
    "        df1.to_csv(\"output_method1.csv\", index=False)\n",
    "        df2.to_csv(\"output_method2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc6f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
