{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaac01f",
   "metadata": {},
   "source": [
    "### Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://huggingface.co/d4data/en_pipeline/resolve/main/en_pipeline-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0697667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (2023.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijaya.sekhar\\anaconda3\\envs\\py10\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets Dbias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a3336",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93659b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31d42a",
   "metadata": {},
   "source": [
    "### input and out path of Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d418dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path_eval = \"C:/Users/vijaya.sekhar/Desktop/accelarators/Bias_Toxicity/bias_test/detox_test_eval.csv\"\n",
    "\n",
    "input_csv_path_dbias = \"C:/Users/vijaya.sekhar/Desktop/accelarators/Bias_Toxicity/bias_test/detox_test_bias.csv\"\n",
    "\n",
    "input_csv_path_classify = \"C:/Users/vijaya.sekhar/Desktop/accelarators/Bias_Toxicity/bias_test/detox_test_classify\"\n",
    "\n",
    "output_csv_path = \"C:/Users/vijaya.sekhar/Desktop/accelarators/Bias_Toxicity/bias_test/detox_test_output1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd93795",
   "metadata": {},
   "source": [
    "### Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58160cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at d4data/bias-detection-model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at d4data/bias-detection-model and are newly initialized: ['dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# References:\n",
    "# https://github.com/dreji18/Fairness-in-AI\n",
    "# https://link.springer.com/article/10.1007/s41060-022-00359-4\n",
    "import pandas as pd\n",
    "from Dbias.bias_classification import *\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\")\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability\n",
    "\n",
    "\n",
    "\n",
    "def calculate_dbias_detection(input_csv_path, output_csv_path):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Iterate through columns and calculate row toxicity for each column\n",
    "    for col_name in df.columns:\n",
    "        # Create new column names for the toxicity scores and labels\n",
    "        new_score_col_name = f\"{col_name}_bias_score\"\n",
    "        new_label_col_name = f\"{col_name}_bias_label\"\n",
    "\n",
    "        # Initialize lists to store scores and labels\n",
    "        toxicity_scores = []\n",
    "        toxicity_labels = []\n",
    "\n",
    "        for text in df[col_name]:\n",
    "            results = classifier(text,padding=True, truncation=True,max_length=512)\n",
    "            classify_toxicity = max(results, key=lambda x: x['score'])\n",
    "\n",
    "            # Append the toxicity score and label\n",
    "#             toxicity_scores.append(classify_toxicity['score'])\n",
    "            toxicity_labels.append(classify_toxicity['label'])\n",
    "            \n",
    "        # Add the toxicity scores and labels as new columns to the DataFrame\n",
    "#         df[new_score_col_name] = toxicity_scores\n",
    "        data_output = pd.read_csv(output_csv_path)\n",
    "        data_output[new_label_col_name] = toxicity_labels\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    data_output.to_csv(output_csv_path,index=False)\n",
    "    print( data_output.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca1e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentences sentences_bias_label\n",
      "0  I'm having issues with my internet connection,...               Biased\n",
      "1  Your service is terrible! And Your prices are ...               Biased\n",
      "2  by playing this game you agree to these terms....           Non-biased\n",
      "3  you have to use google pokemon trainer club or...           Non-biased\n",
      "4  don t die or hurt others and if you do it s no...               Biased\n"
     ]
    }
   ],
   "source": [
    "# Call the function to detect Bias in the sentences\n",
    "calculate_dbias_detection(input_csv_path_dbias, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae19209",
   "metadata": {},
   "source": [
    "### subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be634037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def calculate_classify_subjectivity(input_csv_path, output_csv_path):\n",
    "    # Initialize the text classification pipeline\n",
    "    classify = pipeline(task=\"text-classification\", model=\"cffl/bert-base-styleclassification-subjective-neutral\", padding=True, truncation=True,max_length=512, return_all_scores=True)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Iterate through columns and calculate row toxicity for each column\n",
    "    for col_name in df.columns:\n",
    "        # Create a new column name for the toxicity scores\n",
    "        new_col_name = f\"{col_name}_subjectivity\"\n",
    "\n",
    "        # Calculate toxicity scores and labels for each row in the column\n",
    "        toxicity_scores = []\n",
    "        toxicity_labels = []\n",
    "        for text in df[col_name]:\n",
    "            results = classify(text)\n",
    "            classify_toxicity = max(results[0], key=lambda x: x['score'])['label']\n",
    "            # Find the label with the highest score\n",
    "#             highest_score_label = max(results, key=lambda x: x['score'])\n",
    "#             classify_toxicity = highest_score_label['label']\n",
    "#             classify_score = highest_score_label['score']\n",
    "#             toxicity_scores.append(classify_score)\n",
    "            toxicity_labels.append(classify_toxicity)\n",
    "\n",
    "        # Add the toxicity scores and labels as new columns to the DataFrame\n",
    "#         df[new_col_name + \"_score\"] = toxicity_scores\n",
    "        df[new_col_name + \"_label\"] = toxicity_labels\n",
    "        data_output = pd.read_csv(output_csv_path)\n",
    "        data_output[new_col_name] = toxicity_labels\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    data_output.to_csv(output_csv_path,index=False)\n",
    "    print( data_output.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0172ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to calculate subjectivity\n",
    "calculate_classify_subjectivity(input_csv_path_classify, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71489e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890003a1",
   "metadata": {},
   "source": [
    "### Toxicity and Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f0137663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluate_toxicity(input_csv_path, output_csv_path):\n",
    "        \n",
    "    # Read the CSV \n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    # Define toxicity level bins\n",
    "    bins = [-1, 0.01, 0.1, 0.5, float('inf')]\n",
    "    labels = ['Not Toxic', 'Less Toxic', 'Moderately Toxic', 'Highly Toxic']\n",
    "    \n",
    "    # Iterate through columns for row toxicity for each column ,toxic_label='offensive'\n",
    "    for col_name in df.columns:\n",
    "        eval_toxicity = toxicity.compute(predictions=df[col_name].tolist())['toxicity']\n",
    "        new_col_name = f\"{col_name}_evl_toxicity\"\n",
    "        df[new_col_name] = eval_toxicity\n",
    "        df['toxicity_level'] = pd.cut(df[new_col_name], bins=bins, labels=labels)\n",
    "    data_output = pd.read_csv(output_csv_path)\n",
    "    data_output[new_col_name] = eval_toxicity\n",
    "    data_output[new_col_name] = df['toxicity_level'] \n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    data_output.to_csv(output_csv_path,index=False)\n",
    "    print( data_output.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "580bbb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Unbelievably, with all they had seen their chi...   \n",
      "1  And while states like Massachusetts�among othe...   \n",
      "2  Oxford chose \"climate emergency\" as the word o...   \n",
      "3  In another clip, the group can be seen scrambl...   \n",
      "4  Proponents of stricter protections for student...   \n",
      "\n",
      "  text_dbias_toxicity_label text_cls_toxicity text_evl_toxicity  \n",
      "0                    Biased        SUBJECTIVE         Not Toxic  \n",
      "1                    Biased        SUBJECTIVE         Not Toxic  \n",
      "2                Non-biased           NEUTRAL         Not Toxic  \n",
      "3                Non-biased           NEUTRAL         Not Toxic  \n",
      "4                    Biased        SUBJECTIVE         Not Toxic  \n"
     ]
    }
   ],
   "source": [
    "calculate_evaluate_toxicity(input_csv_path_eval,output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a48fa4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_evaluate_emotion(input_csv_path, output_csv_path):\n",
    "    neutral = [\"This is a neutral tweet\"]\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "\n",
    "    for col_name in df.columns:\n",
    "           \n",
    "        emotions = []\n",
    "        new_col_name = f\"{col_name}_emotion\"\n",
    "        for text in df[col_name]:\n",
    "            reg_dict = regard.compute(data=[text], references=neutral)\n",
    "            reg_dict = pd.DataFrame.from_dict(reg_dict, orient='index').drop(['other'], axis=1).reset_index(drop=True)\n",
    "            reg_dict['emotion'] = reg_dict.idxmax(axis=1)\n",
    "#             print(reg_dict['emotion'].values)\n",
    "            emotions.append(reg_dict['emotion'].values)\n",
    "        \n",
    "            \n",
    "    \n",
    "    data_output = pd.read_csv(output_csv_path)\n",
    "    data_output['emotion_type'] = emotions\n",
    "    print(data_output.head())\n",
    "    data_output.to_csv(output_csv_path, index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "bbb234de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Unbelievably, with all they had seen their chi...   \n",
      "1  And while states like Massachusetts�among othe...   \n",
      "2  Oxford chose \"climate emergency\" as the word o...   \n",
      "3  In another clip, the group can be seen scrambl...   \n",
      "4  Proponents of stricter protections for student...   \n",
      "\n",
      "  text_dbias_toxicity_label text_cls_toxicity text_evl_toxicity emotion_type  \n",
      "0                    Biased        SUBJECTIVE         Not Toxic   [negative]  \n",
      "1                    Biased        SUBJECTIVE         Not Toxic   [negative]  \n",
      "2                Non-biased           NEUTRAL         Not Toxic   [negative]  \n",
      "3                Non-biased           NEUTRAL         Not Toxic   [negative]  \n",
      "4                    Biased        SUBJECTIVE         Not Toxic   [negative]  \n"
     ]
    }
   ],
   "source": [
    "calculate_evaluate_emotion(input_csv_path_eval,output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195c1c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'langkit[all]'\"\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install 'langkit[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bb90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langkit in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (0.0.21)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from langkit) (1.4.4)\n",
      "Requirement already satisfied: textstat<0.8.0,>=0.7.3 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from langkit) (0.7.3)\n",
      "Requirement already satisfied: whylogs<2.0.0,>=1.3.9 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from langkit) (1.3.9)\n",
      "Requirement already satisfied: pyphen in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from textstat<0.8.0,>=0.7.3->langkit) (0.14.0)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.5.0 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (3.11.0)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (4.22.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.27 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (2.28.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.30.0.0 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (2.31.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.10 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (4.3.0)\n",
      "Requirement already satisfied: whylabs-client<0.6.0,>=0.5.6 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (0.5.8)\n",
      "Requirement already satisfied: whylogs-sketching>=3.4.1.dev3 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from whylogs<2.0.0,>=1.3.9->langkit) (3.4.1.dev3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from pandas->langkit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from pandas->langkit) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from pandas->langkit) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->langkit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.27->whylogs<2.0.0,>=1.3.9->langkit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.27->whylogs<2.0.0,>=1.3.9->langkit) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.27->whylogs<2.0.0,>=1.3.9->langkit) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.27->whylogs<2.0.0,>=1.3.9->langkit) (2022.9.14)\n",
      "Requirement already satisfied: types-urllib3 in c:\\users\\vijaya.sekhar\\anaconda3\\lib\\site-packages (from types-requests<3.0.0.0,>=2.30.0.0->whylogs<2.0.0,>=1.3.9->langkit) (1.26.25.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ed253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
    "import whylogs as why\n",
    "\n",
    "why.init(session_type='whylabs_anonymous')\n",
    "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
    "schema = llm_metrics.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955fa91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
