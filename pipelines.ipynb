{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b167978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Optional, Dict, Union\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import torch\n",
    "from transformers import(\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QGPipeline:\n",
    "    \"\"\"Poor man's QG pipeline\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        ans_model: PreTrainedModel,\n",
    "        ans_tokenizer: PreTrainedTokenizer,\n",
    "        qg_format: str,\n",
    "        use_cuda: bool\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.ans_model = ans_model\n",
    "        self.ans_tokenizer = ans_tokenizer\n",
    "\n",
    "        self.qg_format = qg_format\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.ans_model is not self.model:\n",
    "            self.ans_model.to(self.device)\n",
    "\n",
    "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
    "        \n",
    "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
    "            self.model_type = \"t5\"\n",
    "        else:\n",
    "            self.model_type = \"bart\"\n",
    "\n",
    "    def __call__(self, inputs: str):\n",
    "        inputs = \" \".join(inputs.split())\n",
    "        sents, answers = self._extract_answers(inputs)\n",
    "        flat_answers = list(itertools.chain(*answers))\n",
    "        \n",
    "        if len(flat_answers) == 0:\n",
    "          return []\n",
    "\n",
    "        if self.qg_format == \"prepend\":\n",
    "            qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
    "        else:\n",
    "            qg_examples = self._prepare_inputs_for_qg_from_answers_hl(sents, answers)\n",
    "        \n",
    "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
    "        questions = self._generate_questions(qg_inputs)\n",
    "        output = [{'answer': example['answer'], 'question': que} for example, que in zip(qg_examples, questions)]\n",
    "        return output\n",
    "    \n",
    "    def _generate_questions(self, inputs):\n",
    "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
    "        \n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device), \n",
    "            attention_mask=inputs['attention_mask'].to(self.device), \n",
    "            max_length=32,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        \n",
    "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        return questions\n",
    "    \n",
    "    def _extract_answers(self, context):\n",
    "        sents, inputs = self._prepare_inputs_for_ans_extraction(context)\n",
    "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
    "\n",
    "        outs = self.ans_model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device), \n",
    "            attention_mask=inputs['attention_mask'].to(self.device), \n",
    "            max_length=32,\n",
    "        )\n",
    "        \n",
    "        dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
    "        answers = [item.split('<sep>') for item in dec]\n",
    "        answers = [i[:-1] for i in answers]\n",
    "        \n",
    "        return sents, answers\n",
    "    \n",
    "    def _tokenize(self,\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512\n",
    "    ):\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            inputs, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=truncation,\n",
    "            padding=\"max_length\" if padding else False,\n",
    "            pad_to_max_length=padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_inputs_for_ans_extraction(self, text):\n",
    "        sents = sent_tokenize(text)\n",
    "\n",
    "        inputs = []\n",
    "        for i in range(len(sents)):\n",
    "            source_text = \"extract answers:\"\n",
    "            for j, sent in enumerate(sents):\n",
    "                if i == j:\n",
    "                    sent = \"<hl> %s <hl>\" % sent\n",
    "                source_text = \"%s %s\" % (source_text, sent)\n",
    "                source_text = source_text.strip()\n",
    "            \n",
    "            if self.model_type == \"t5\":\n",
    "                source_text = source_text + \" </s>\"\n",
    "            inputs.append(source_text)\n",
    "\n",
    "        return sents, inputs\n",
    "    \n",
    "    def _prepare_inputs_for_qg_from_answers_hl(self, sents, answers):\n",
    "        inputs = []\n",
    "        for i, answer in enumerate(answers):\n",
    "            if len(answer) == 0: continue\n",
    "            for answer_text in answer:\n",
    "                sent = sents[i]\n",
    "                sents_copy = sents[:]\n",
    "                \n",
    "                answer_text = answer_text.strip()\n",
    "                \n",
    "                ans_start_idx = sent.index(answer_text)\n",
    "                \n",
    "                sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
    "                sents_copy[i] = sent\n",
    "                \n",
    "                source_text = \" \".join(sents_copy)\n",
    "                source_text = f\"generate question: {source_text}\" \n",
    "                if self.model_type == \"t5\":\n",
    "                    source_text = source_text + \" </s>\"\n",
    "                \n",
    "                inputs.append({\"answer\": answer_text, \"source_text\": source_text})\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_inputs_for_qg_from_answers_prepend(self, context, answers):\n",
    "        flat_answers = list(itertools.chain(*answers))\n",
    "        examples = []\n",
    "        for answer in flat_answers:\n",
    "            source_text = f\"answer: {answer} context: {context}\"\n",
    "            if self.model_type == \"t5\":\n",
    "                source_text = source_text + \" </s>\"\n",
    "            \n",
    "            examples.append({\"answer\": answer, \"source_text\": source_text})\n",
    "        return examples\n",
    "\n",
    "    \n",
    "class MultiTaskQAQGPipeline(QGPipeline):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def __call__(self, inputs: Union[Dict, str]):\n",
    "        if type(inputs) is str:\n",
    "            # do qg\n",
    "            return super().__call__(inputs)\n",
    "        else:\n",
    "            # do qa\n",
    "            return self._extract_answer(inputs[\"question\"], inputs[\"context\"])\n",
    "    \n",
    "    def _prepare_inputs_for_qa(self, question, context):\n",
    "        source_text = f\"question: {question}  context: {context}\"\n",
    "        if self.model_type == \"t5\":\n",
    "            source_text = source_text + \" </s>\"\n",
    "        return  source_text\n",
    "    \n",
    "    def _extract_answer(self, question, context):\n",
    "        source_text = self._prepare_inputs_for_qa(question, context)\n",
    "        inputs = self._tokenize([source_text], padding=False)\n",
    "    \n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device), \n",
    "            attention_mask=inputs['attention_mask'].to(self.device), \n",
    "            max_length=16,\n",
    "        )\n",
    "\n",
    "        answer = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class E2EQGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        use_cuda: bool\n",
    "    ) :\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
    "        \n",
    "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
    "            self.model_type = \"t5\"\n",
    "        else:\n",
    "            self.model_type = \"bart\"\n",
    "        \n",
    "        self.default_generate_kwargs = {\n",
    "            \"max_length\": 256,\n",
    "            \"num_beams\": 4,\n",
    "            \"length_penalty\": 1.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "    \n",
    "    def __call__(self, context: str, **generate_kwargs):\n",
    "        inputs = self._prepare_inputs_for_e2e_qg(context)\n",
    "\n",
    "        # TODO: when overrding default_generate_kwargs all other arguments need to be passsed\n",
    "        # find a better way to do this\n",
    "        if not generate_kwargs:\n",
    "            generate_kwargs = self.default_generate_kwargs\n",
    "        \n",
    "        input_length = inputs[\"input_ids\"].shape[-1]\n",
    "        \n",
    "        # max_length = generate_kwargs.get(\"max_length\", 256)\n",
    "        # if input_length < max_length:\n",
    "        #     logger.warning(\n",
    "        #         \"Your max_length is set to {}, but you input_length is only {}. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\".format(\n",
    "        #             max_length, input_length\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device), \n",
    "            attention_mask=inputs['attention_mask'].to(self.device),\n",
    "            **generate_kwargs\n",
    "        )\n",
    "\n",
    "        prediction = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        questions = prediction.split(\"<sep>\")\n",
    "        questions = [question.strip() for question in questions[:-1]]\n",
    "        return questions\n",
    "    \n",
    "    def _prepare_inputs_for_e2e_qg(self, context):\n",
    "        source_text = f\"generate questions: {context}\"\n",
    "        if self.model_type == \"t5\":\n",
    "            source_text = source_text + \" </s>\"\n",
    "        \n",
    "        inputs = self._tokenize([source_text], padding=False)\n",
    "        return inputs\n",
    "    \n",
    "    def _tokenize(\n",
    "        self,\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512\n",
    "    ):\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            inputs, \n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=truncation,\n",
    "            padding=\"max_length\" if padding else False,\n",
    "            pad_to_max_length=padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "\n",
    "SUPPORTED_TASKS = {\n",
    "    \"question-generation\": {\n",
    "        \"impl\": QGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-qg-hl\",\n",
    "            \"ans_model\": \"valhalla/t5-small-qa-qg-hl\",\n",
    "        }\n",
    "    },\n",
    "    \"multitask-qa-qg\": {\n",
    "        \"impl\": MultiTaskQAQGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-qa-qg-hl\",\n",
    "        }\n",
    "    },\n",
    "    \"e2e-qg\": {\n",
    "        \"impl\": E2EQGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-e2e-qg\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
    "    qg_format: Optional[str] = \"highlight\",\n",
    "    ans_model: Optional = None,\n",
    "    ans_tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
    "    use_cuda: Optional[bool] = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Retrieve the task\n",
    "    if task not in SUPPORTED_TASKS:\n",
    "        raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\n",
    "\n",
    "    targeted_task = SUPPORTED_TASKS[task]\n",
    "    task_class = targeted_task[\"impl\"]\n",
    "\n",
    "    # Use default model/config/tokenizer for the task if no model is provided\n",
    "    if model is None:\n",
    "        model = targeted_task[\"default\"][\"model\"]\n",
    "    \n",
    "    # Try to infer tokenizer from model or config name (if provided as str)\n",
    "    if tokenizer is None:\n",
    "        if isinstance(model, str):\n",
    "            tokenizer = model\n",
    "        else:\n",
    "            # Impossible to guest what is the right tokenizer here\n",
    "            raise Exception(\n",
    "                \"Impossible to guess which tokenizer to use. \"\n",
    "                \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
    "            )\n",
    "    \n",
    "    # Instantiate tokenizer if needed\n",
    "    if isinstance(tokenizer, (str, tuple)):\n",
    "        if isinstance(tokenizer, tuple):\n",
    "            # For tuple we have (tokenizer name, {kwargs})\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer[0], **tokenizer[1])\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    \n",
    "    # Instantiate model if needed\n",
    "    if isinstance(model, str):\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "    \n",
    "    if task == \"question-generation\":\n",
    "        if ans_model is None:\n",
    "            # load default ans model\n",
    "            ans_model = targeted_task[\"default\"][\"ans_model\"]\n",
    "            ans_tokenizer = AutoTokenizer.from_pretrained(ans_model)\n",
    "            ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
    "        else:\n",
    "            # Try to infer tokenizer from model or config name (if provided as str)\n",
    "            if ans_tokenizer is None:\n",
    "                if isinstance(ans_model, str):\n",
    "                    ans_tokenizer = ans_model\n",
    "                else:\n",
    "                    # Impossible to guest what is the right tokenizer here\n",
    "                    raise Exception(\n",
    "                        \"Impossible to guess which tokenizer to use. \"\n",
    "                        \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
    "                    )\n",
    "            \n",
    "            # Instantiate tokenizer if needed\n",
    "            if isinstance(ans_tokenizer, (str, tuple)):\n",
    "                if isinstance(ans_tokenizer, tuple):\n",
    "                    # For tuple we have (tokenizer name, {kwargs})\n",
    "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer[0], **ans_tokenizer[1])\n",
    "                else:\n",
    "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer)\n",
    "\n",
    "            if isinstance(ans_model, str):\n",
    "                ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
    "    \n",
    "    if task == \"e2e-qg\":\n",
    "        return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)\n",
    "    elif task == \"question-generation\":\n",
    "        return task_class(model=model, tokenizer=tokenizer, ans_model=ans_model, ans_tokenizer=ans_tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
    "    else:\n",
    "        return task_class(model=model, tokenizer=tokenizer, ans_model=model, ans_tokenizer=tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
